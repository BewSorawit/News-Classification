{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('stopwords',quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('omw-1.4',quiet=True)\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import  LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade numpy==1.23.5\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english')\n",
    "default_stopwords = default_stopwords + ['said', 'would','even','according','could','year',\n",
    "                                         'years','also','new','people','old,''one','two','time',\n",
    "                                         'first','last','say','make','best','get','three','make',\n",
    "                                         'year old','told','made','like','take','many','set','number',\n",
    "                                         'month','week','well','back']\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,4}\\b\\d')\n",
    "BAD_SYMBOLS_RE = re.compile(\"[^a-zA-Z,\\d]\")\n",
    "REPLACE_IP_ADDRESS = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "def clean_text(text, ):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s) if len(w)>=3]\n",
    "\n",
    "    def preprocessing_text(text):\n",
    "        text = text.lower()\n",
    "        text=text.replace('\\n',' ').replace('\\xa0',' ').replace('-',' ').replace('รณ','o').replace('ฤ','g').replace('รก','a').replace(\"'\",\" \")\n",
    "        text=re.sub(r'\\d+','', text)\n",
    "        text=re.sub(r'http\\S+', '', text)\n",
    "        text=BAD_SYMBOLS_RE.sub(' ', text)\n",
    "        text=REPLACE_IP_ADDRESS.sub('', text)\n",
    "        text=REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "        text=' '.join(word for word in text.split() if len(word)>3)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def remove_special_characters(text, characters=string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters + '0123456789')))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def lemm_text(text, lemm=WordNetLemmatizer()):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([lemm.lemmatize(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = text.lower() # lowercase\n",
    "    #text = stem_text(text) # stemming\n",
    "    text=preprocessing_text(text)\n",
    "    text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    text = lemm_text(text) # lemmatizer\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('MN-DS-news-classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['category_level_1'].isin(['sport', 'science and technology', 'economy, business and finance', 'arts, culture, entertainment and media','politics', ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']= df[['title', 'content']].apply(lambda x: ' . '.join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(y):\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertModel: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "layer = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer,  maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        truncation=True,\n",
    "        return_attention_mask=False, \n",
    "        return_token_type_ids=False,\n",
    "        padding=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, y_shape, loss='categorical_crossentropy', max_len=512):\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    #adding dropout layer\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.3)(cls_token)\n",
    "\n",
    "    #using a dense layer of category size neurons. \n",
    "    out = tf.keras.layers.Dense(y_shape, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n",
    "\n",
    "    #using categorical crossentropy as the loss as it is a multi-class classification problem\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=5e-5), loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dist_bert(X, y, model_save_path):\n",
    "    \n",
    "    y_lr = encode_labels(y)\n",
    "\n",
    "    #converting the categories into one hot vectors using tf.keras.utils.to_categorical\n",
    "    y_cat = tf.keras.utils.to_categorical(y_lr, dtype = 'int32')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_cat, random_state = 42, test_size = 0.2)\n",
    "    \n",
    "    #building the model\n",
    "    model = build_model(layer, max_len=80, y_shape=y_cat.shape[1])\n",
    "    \n",
    "    #creating the training and testing dataset.\n",
    "    BATCH_SIZE = 32\n",
    "    AUTO = tf.data.experimental.AUTOTUNE \n",
    "    train_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((X_train, y_train))\n",
    "        .repeat()\n",
    "        .shuffle(2048)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    test_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(X_test)\n",
    "        .batch(BATCH_SIZE)\n",
    "    )\n",
    "    \n",
    "    #training for 10 epochs\n",
    "    n_steps = X_train.shape[0] // BATCH_SIZE\n",
    "    train_history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=n_steps,\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    #making predictions\n",
    "    preds = model.predict(test_dataset,verbose = 1)\n",
    "    #converting the one hot vector output to a linear numpy array.\n",
    "    pred_classes = np.argmax(preds, axis = 1)\n",
    "    \n",
    "    print('Precision is {}'.format(precision_score(np.argmax(y_test, axis=1), pred_classes, average='macro')))\n",
    "    print('Recall is {}'.format(recall_score(np.argmax(y_test, axis=1), pred_classes, average='macro')))\n",
    "    print('F1:', f1_score(np.argmax(y_test, axis=1), pred_classes, average='macro'))\n",
    "\n",
    "    model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = regular_encode(df['text'].astype('str'), tokenizer, maxlen=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 23s 163ms/step - loss: 0.6012 - accuracy: 0.7721\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 13s 163ms/step - loss: 0.1839 - accuracy: 0.9432\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 13s 164ms/step - loss: 0.1022 - accuracy: 0.9684\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 13s 164ms/step - loss: 0.0666 - accuracy: 0.9832\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 13s 164ms/step - loss: 0.0361 - accuracy: 0.9882\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 13s 164ms/step - loss: 0.0433 - accuracy: 0.9855\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 14s 165ms/step - loss: 0.0117 - accuracy: 0.9966\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 14s 165ms/step - loss: 0.0147 - accuracy: 0.9958\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 14s 168ms/step - loss: 0.0127 - accuracy: 0.9966\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 14s 165ms/step - loss: 0.0102 - accuracy: 0.9966\n",
      "21/21 [==============================] - 3s 60ms/step\n",
      "Precision is 0.9024782678556837\n",
      "Recall is 0.9070667575993234\n",
      "F1: 0.9042106121133706\n"
     ]
    }
   ],
   "source": [
    "train_dist_bert(X_encoded, df['category_level_1'], 'bjk/model_category_level_1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "272/272 [==============================] - 53s 164ms/step - loss: 1.6258 - accuracy: 0.5893\n",
      "Epoch 2/10\n",
      "272/272 [==============================] - 51s 188ms/step - loss: 0.5854 - accuracy: 0.8241\n",
      "Epoch 3/10\n",
      "272/272 [==============================] - 47s 173ms/step - loss: 0.4119 - accuracy: 0.8747\n",
      "Epoch 4/10\n",
      "272/272 [==============================] - 46s 169ms/step - loss: 0.2825 - accuracy: 0.9151\n",
      "Epoch 5/10\n",
      "272/272 [==============================] - 46s 170ms/step - loss: 0.2277 - accuracy: 0.9318\n",
      "Epoch 6/10\n",
      "272/272 [==============================] - 47s 172ms/step - loss: 0.1995 - accuracy: 0.9404\n",
      "Epoch 7/10\n",
      "272/272 [==============================] - 46s 170ms/step - loss: 0.1713 - accuracy: 0.9467\n",
      "Epoch 8/10\n",
      "272/272 [==============================] - 47s 173ms/step - loss: 0.1629 - accuracy: 0.9463\n",
      "Epoch 9/10\n",
      "272/272 [==============================] - 47s 171ms/step - loss: 0.1544 - accuracy: 0.9480\n",
      "Epoch 10/10\n",
      "272/272 [==============================] - 45s 166ms/step - loss: 0.1486 - accuracy: 0.9499\n",
      "69/69 [==============================] - 5s 57ms/step\n",
      "Precision is 0.7354813154570313\n",
      "Recall is 0.7302324414296032\n",
      "F1: 0.7251995231706533\n"
     ]
    }
   ],
   "source": [
    "# train_dist_bert(X_encoded, df['category_level_2'], 'bjk/model_category_level_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# เธฅเนเธฒเธเธซเธเนเธงเธขเธเธงเธฒเธกเธเธณ\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# เนเธเน CPU เนเธเธ GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# เนเธเน eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# เนเธซเธฅเธ tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# เนเธซเธฅเธเนเธกเนเธเธฅ\n",
    "model_category_level_1 = tf.keras.models.load_model(\n",
    "    'bjk/model_category_level_1.h5',\n",
    "    custom_objects={'TFDistilBertModel': TFDistilBertModel}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = [\n",
    "    \"s we have said, Pakistan have opted to play this match on last week's pitch to potentially bring their spinners into the game. They have picked seven frontline and part-time spin options after all. But the data from our friends as CricViz suggests that may not necessarily be playing to their strengths. Since cricket returned to Pakistan in 2019 their spinners average 45.8 compared 37.3 by the seamers. In fact, Pakistan's spin record at home is the second worst of all of the Test-playing hosts in that time. There isn't as much spin bowled in Pakistan domestic cricket as you might expect either. 37.4percent of overs in Pakistan's first-class domestic cricket have been bowled by spinners in the past 10 years. That number is far higher in Sri Lanka (67.6%) and Bangladesh, Afghanistan and West Indies too who more than half of the overs are bowled by spinners. The figure is 42.9percent in India.\",\n",
    "    \"Another news report focusing on technology advancements.\"\n",
    "]\n",
    "\n",
    "# เธเธณเธซเธเธ maxlen เธเธฒเธกเธเธตเนเนเธเนเนเธเธเธเธฐเธเธถเธเนเธกเนเธเธฅ\n",
    "maxlen = 80\n",
    "\n",
    "# เธเธฑเธเธเนเธเธฑเธเนเธเนเธฒเธฃเธซเธฑเธชเธเนเธญเธเธงเธฒเธกเนเธซเธกเน\n",
    "def regular_encode(texts, tokenizer, maxlen=80):\n",
    "    tokens = tokenizer(texts, padding='max_length', truncation=True, max_length=maxlen, return_tensors='tf')\n",
    "    return tokens['input_ids']\n",
    "\n",
    "# เนเธเนเธฒเธฃเธซเธฑเธชเธเนเธญเธเธงเธฒเธกเนเธซเธกเน\n",
    "X_new_encoded = regular_encode(new_texts, tokenizer, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_new_encoded: (2, 80)\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "1.7915208\n",
      "[0.36692008 0.05742038 0.13207321 0.24074513 0.994362  ]\n",
      "0.36692008\n",
      "[0.36692008 0.05742038 0.13207321 0.24074513 0.        ]\n",
      "[0 3]\n",
      "Predicted Classes for Category Level 1: [4 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_new_encoded:\", X_new_encoded.shape)\n",
    "\n",
    "predictions_level_1 = model_category_level_1.predict(X_new_encoded)\n",
    "# predictions_level_2 = model_category_level_2.predict(X_new_encoded)\n",
    "\n",
    "# เนเธชเธเธเธเธฅเธเธฒเธฃเธเธณเธเธฒเธข\n",
    "predicted_classes_level_1 = np.argmax(predictions_level_1, axis=1)\n",
    "# predicted_classes_level_2 = np.argmax(predictions_level_2, axis=1)\n",
    "print(predictions_level_1[0].sum())\n",
    "print(predictions_level_1[0])\n",
    "predictions_level_1[0][np.argmax(predictions_level_1, axis=1)[0]] = 0\n",
    "print(predictions_level_1[0][np.argmax(predictions_level_1, axis=1)[0]])\n",
    "print(predictions_level_1[0])\n",
    "print(np.argmax(predictions_level_1, axis=1))\n",
    "\n",
    "print(\"Predicted Classes for Category Level 1:\", predicted_classes_level_1)\n",
    "# print(\"Predicted Classes for Category Level 2:\", predicted_classes_level_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCategory2(find):\n",
    "  grouped_counts = df.groupby(['category_level_1'])['data_id'].agg('count')\n",
    "  unique_categories = grouped_counts.index.tolist()\n",
    "  if len(unique_categories) >= find:\n",
    "      category = unique_categories[find]  # Index 26 corresponds to the 27th element\n",
    "      count = grouped_counts.loc[category]\n",
    "      print(f\"Count for category '{category}': {count}\")\n",
    "  else:\n",
    "      print(\"There are fewer than \",find,\" unique categories in 'category_level_1'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_level_1\n",
       "arts, culture, entertainment and media    300\n",
       "economy, business and finance             400\n",
       "politics                                  900\n",
       "science and technology                    800\n",
       "sport                                     907\n",
       "Name: data_id, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['category_level_1'])['data_id'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for category 'sport': 907\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(findCategory2(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-1387.7538, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available :  1\n",
      "GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# เธเธฃเธงเธเธชเธญเธเธงเนเธฒ TensorFlow เนเธเน GPU เธซเธฃเธทเธญ CPU\n",
    "print(\"Num GPUs Available : \", len(tf.config.list_physical_devices('GPU')))\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs Available: \", len(gpus))\n",
    "else:\n",
    "    print(\"No GPUs available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14146279233132396968\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3655335936\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3452987027427254167\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Transformers version: 4.21.1\n",
      "64_8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(tf.sysconfig.get_build_info()['cudnn_version'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
