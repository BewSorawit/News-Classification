{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8iXbPgUCMyv"
   },
   "source": [
    "# Imports / Dataloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiD_0MbIpPmy",
    "outputId": "3fbed9ba-eaf0-44eb-870f-2079a3ce52b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jakkapan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jakkapan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jakkapan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas matplotlib seaborn nltk scikit-learn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpFOvBZiy1xK",
    "outputId": "1ea7f783-239f-46ff-84dd-7ff53600c59b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jakkapan\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xzyaQSXxJdOy"
   },
   "outputs": [],
   "source": [
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english')\n",
    "default_stopwords = default_stopwords + ['said', 'would','even','according','could','year',\n",
    "                                         'years','also','new','people','old,''one','two','time',\n",
    "                                         'first','last','say','make','best','get','three','make',\n",
    "                                         'year old','told','made','like','take','many','set','number',\n",
    "                                         'month','week','well','back']\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,4}\\b\\d')\n",
    "BAD_SYMBOLS_RE = re.compile(\"[^a-zA-Z,\\d]\")\n",
    "REPLACE_IP_ADDRESS = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "def clean_text(text, ):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s) if len(w)>=3]\n",
    "\n",
    "    def preprocessing_text(text):\n",
    "        text = text.lower()\n",
    "        text=text.replace('\\n',' ').replace('\\xa0',' ').replace('-',' ').replace('รณ','o').replace('ฤ','g').replace('รก','a').replace(\"'\",\" \")\n",
    "        text=re.sub(r'\\d+','', text)\n",
    "        text=re.sub(r'http\\S+', '', text)\n",
    "        text=BAD_SYMBOLS_RE.sub(' ', text)\n",
    "        text=REPLACE_IP_ADDRESS.sub('', text)\n",
    "        text=REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "        text=' '.join(word for word in text.split() if len(word)>3)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def remove_special_characters(text, characters=string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters + '0123456789')))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def lemm_text(text, lemm=WordNetLemmatizer()):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([lemm.lemmatize(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = text.lower() # lowercase\n",
    "    #text = stem_text(text) # stemming\n",
    "    text=preprocessing_text(text)\n",
    "    text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    text = lemm_text(text) # lemmatizer\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4kW7MxLeRb8F"
   },
   "outputs": [],
   "source": [
    "# pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "J9zSdr7I7f7L",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('MN-DS-news-classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PntgndCxEU5c"
   },
   "source": [
    "Articles number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDK9r5XDEQeO",
    "outputId": "bb4ad11e-c867-4352-9a6c-49e121000398"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10917"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bD66QxcBSx6a"
   },
   "source": [
    "Category number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSKPodzaSMlA",
    "outputId": "677392b1-ff2f-45d7-f94a-96785b004ace"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 109)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_level_1.nunique(), df.category_level_2.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocnOG4ZPE3vV"
   },
   "source": [
    " Media sources number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWUNHlktE4D0",
    "outputId": "d48b8252-8d04-4eeb-c1e0-4bf4aa1e6eab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7JK0yfoPQNQ"
   },
   "source": [
    "Review transfered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5sdkXT1RYRn",
    "outputId": "26e8a1ed-1c83-4330-ba53-f308982c3b38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['data_id', 'id', 'date', 'source', 'title', 'content', 'author', 'url',\n",
       "       'published', 'published_utc', 'collection_utc', 'category_level_1',\n",
       "       'category_level_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7rr3aEmHhtD"
   },
   "source": [
    "The number of articles under each Level 1 category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vl5xHuIrHh7A",
    "outputId": "2d4e2ff3-9a32-4627-db72-d7cc288cbc0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_level_1\n",
       "arts, culture, entertainment and media        300\n",
       "conflict, war and peace                       800\n",
       "crime, law and justice                        500\n",
       "disaster, accident and emergency incident     500\n",
       "economy, business and finance                 400\n",
       "education                                     607\n",
       "environment                                   600\n",
       "health                                        700\n",
       "human interest                                600\n",
       "labour                                        703\n",
       "lifestyle and leisure                         300\n",
       "politics                                      900\n",
       "religion and belief                           800\n",
       "science and technology                        800\n",
       "society                                      1100\n",
       "sport                                         907\n",
       "weather                                       400\n",
       "Name: data_id, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['category_level_1'])['data_id'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "g2LaQ0FMHiCT"
   },
   "outputs": [],
   "source": [
    "df['combined_categories']= df[['category_level_1', 'category_level_2']].apply(lambda x: ' . '.join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "z1Cg1Y1xHiGm"
   },
   "outputs": [],
   "source": [
    "df['text']= df[['title', 'content']].apply(lambda x: ' . '.join(x.astype(str)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text tokeniztion and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1o7zj_0HEza"
   },
   "source": [
    "Word cloud of MN-DS dataset for selected second-level categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "g4TN4hC7JVyX"
   },
   "outputs": [],
   "source": [
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=default_stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(title, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yKKR4FO7GZZk",
    "outputId": "135c36cf-9a28-4608-d0d8-b8b6a99928b5"
   },
   "outputs": [],
   "source": [
    "df.groupby('category_level_2').apply(\n",
    "    lambda x: show_wordcloud(x.text.tolist(), title=f\"Category level 2: {x.name}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kELhyyh6HHxO"
   },
   "source": [
    "Mean number of non-repeated words in article body for first-level categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VP-ntX5LGZc_"
   },
   "outputs": [],
   "source": [
    "# Add column 'len' => number of words in the article (content+title)\n",
    "def counter(x):\n",
    "    return len(set(x.split(' ')))\n",
    "df['len'] = df.text.apply(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "pYA94RsILSFQ",
    "outputId": "7775e7bb-5473-4cb6-f31e-aa889f39a744"
   },
   "outputs": [],
   "source": [
    "catpl = sns.catplot(data=df, kind=\"bar\", x=\"len\", y=\"category_level_1\", height=9, aspect=12/9)\n",
    "sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\n",
    "catpl.set_titles('Mean number of words in article body for each category', fontsize=16);\n",
    "catpl.set_xlabels('Number of words');\n",
    "catpl.set_ylabels('Word count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ate0UdYqHJ-e"
   },
   "source": [
    "## Multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb(X, y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "    \n",
    "    # Define a pipeline combining a text feature extractor with multi lable classifier\n",
    "    NB_pipeline = Pipeline([\n",
    "                    ('clf', OneVsRestClassifier(MultinomialNB())),\n",
    "                ])\n",
    "\n",
    "    NB_pipeline.fit(X_train, y_train)\n",
    "    # compute the testing accuracy\n",
    "    prediction = NB_pipeline.predict(X_test)\n",
    "    print('Precision is {}'.format(precision_score(y_test, prediction, average='macro')))\n",
    "    print('Recall is {}'.format(recall_score(y_test, prediction, average='macro')))\n",
    "    print('F1:', f1_score(y_test, prediction, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr(X, y):\n",
    "    y = encode_labels(y)\n",
    "\n",
    "    X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "    # Using pipeline for applying logistic regression and one vs rest classifier\n",
    "    model = OneVsRestClassifier(LogisticRegression(solver='sag'))\n",
    "\n",
    "    # Fitting the model with training data\n",
    "    model.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "    # Making a prediction on the test set\n",
    "    prediction = model.predict(X_test_lr)\n",
    "\n",
    "    # Evaluating the model\n",
    "    print('Precision is {}'.format(precision_score(y_test_lr, prediction, average='macro')))\n",
    "    print('Recall is {}'.format(recall_score(y_test_lr, prediction, average='macro')))\n",
    "    print('F1:', f1_score(y_test_lr, prediction, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svc(X, y):\n",
    "    \n",
    "    y = encode_labels(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "    \n",
    "    # Creating the SVM model\n",
    "    model = OneVsRestClassifier(SVC())\n",
    "\n",
    "    # Fitting the model with training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Making a prediction on the test set\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    # Evaluating the model\n",
    "    print('Precision is {}'.format(precision_score(y_test, prediction, average='macro')))\n",
    "    print('Recall is {}'.format(recall_score(y_test, prediction, average='macro')))\n",
    "    print('F1:', f1_score(y_test, prediction, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(X):\n",
    "    # Use TfidfVectorizer. Output: Vectors for train text.\n",
    "    vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,2), norm='l2', max_features=50000)\n",
    "    vectors = vectorizer.fit_transform(X)\n",
    "\n",
    "    return vectors.toarray()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(y):\n",
    "    le = LabelEncoder()\n",
    "    return le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = get_tfidf(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5HNJf_S7f7i"
   },
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 1 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb(X_tfidf, df['category_level_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb(X_tfidf, df['category_level_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-iCsejp7f7j"
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh--oQLz7f7j"
   },
   "source": [
    "Level 1 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lr(X_tfidf, df['category_level_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lr(X_tfidf, df['category_level_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwCiTxNA7f7k"
   },
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 1 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svc(X_tfidf, df['category_level_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svc(X_tfidf, df['category_level_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6aP1dqy7f7l"
   },
   "source": [
    "### glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2YZuSCyYHqT"
   },
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tel-q7WLYHhV"
   },
   "outputs": [],
   "source": [
    "# !unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "xmiE2ET5bCig"
   },
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKy4JVbB7f7m"
   },
   "outputs": [],
   "source": [
    "embedding = loadGloveModel('glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "CNwCoV7s7f7m"
   },
   "outputs": [],
   "source": [
    "def mean_vector(data):\n",
    "    vec_list = []\n",
    "    for sting in data:\n",
    "        l = 0\n",
    "        vector = np.zeros(300)\n",
    "        for word in sting:\n",
    "            try:\n",
    "                vector += embedding[word]\n",
    "                l += 1\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        vector = vector / l\n",
    "        vec_list.append(vector)\n",
    "    return vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "CReMBDFn7f7m"
   },
   "outputs": [],
   "source": [
    "def preprocces(X):\n",
    "    X_proccessed = []\n",
    "    for x in X:\n",
    "        x = word_tokenize(x)\n",
    "        X_proccessed.append(x)\n",
    "    return X_proccessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPwFS3_07f7m",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "X_preprocessed = preprocces(df['text'])\n",
    "vectors = mean_vector(X_preprocessed)\n",
    "X_tokens = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "iE5Ozr227f7n"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_glove = scaler.fit_transform(X_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4YKDBnl7f7n"
   },
   "source": [
    "Level 1 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb(X_glove, df['category_level_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb(X_glove, df['category_level_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wNEPeci7f7o"
   },
   "source": [
    "#### Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rUuPHJc7f7o",
    "scrolled": true
   },
   "source": [
    "Level 1 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lr(X_glove, df['category_level_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lr(X_glove, df['category_level_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKH8TXGt7f7p"
   },
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqaOZkFF7f7q"
   },
   "source": [
    "Level 1 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svc(X_glove, df['category_level_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svc(X_glove, df['category_level_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9t9FE0c7f7r"
   },
   "source": [
    "# DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPXwuwOp7f7r"
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "layer = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "vJNLjl3G7f7s"
   },
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer,  maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        truncation=True,\n",
    "        return_attention_mask=False, \n",
    "        return_token_type_ids=False,\n",
    "        padding=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, y_shape, loss='categorical_crossentropy', max_len=512):\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    #adding dropout layer\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.3)(cls_token)\n",
    "\n",
    "    #using a dense layer of category size neurons. \n",
    "    out = tf.keras.layers.Dense(y_shape, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n",
    "\n",
    "    #using categorical crossentropy as the loss as it is a multi-class classification problem\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=5e-5), loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dist_bert(X, y):\n",
    "    \n",
    "    y_lr = encode_labels(y)\n",
    "\n",
    "    #converting the categories into one hot vectors using tf.keras.utils.to_categorical\n",
    "    y_cat = tf.keras.utils.to_categorical(y_lr, dtype = 'int32')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_cat, random_state = 42, test_size = 0.2)\n",
    "    \n",
    "    #building the model\n",
    "    model = build_model(layer, max_len=80, y_shape=y_cat.shape[1])\n",
    "    \n",
    "    #creating the training and testing dataset.\n",
    "    BATCH_SIZE = 32\n",
    "    AUTO = tf.data.experimental.AUTOTUNE \n",
    "    train_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices((X_train, y_train))\n",
    "        .repeat()\n",
    "        .shuffle(2048)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(AUTO)\n",
    "    )\n",
    "    test_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(X_test)\n",
    "        .batch(BATCH_SIZE)\n",
    "    )\n",
    "    \n",
    "    #training for 10 epochs\n",
    "    n_steps = X_train.shape[0] // BATCH_SIZE\n",
    "    train_history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=n_steps,\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    #making predictions\n",
    "    preds = model.predict(test_dataset,verbose = 1)\n",
    "    #converting the one hot vector output to a linear numpy array.\n",
    "    pred_classes = np.argmax(preds, axis = 1)\n",
    "    \n",
    "    print('Precision is {}'.format(precision_score(np.argmax(y_test, axis=1), pred_classes, average='macro')))\n",
    "    print('Recall is {}'.format(recall_score(np.argmax(y_test, axis=1), pred_classes, average='macro')))\n",
    "    print('F1:', f1_score(np.argmax(y_test, axis=1), pred_classes, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = regular_encode(df['text'].astype('str'), tokenizer, maxlen=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 1 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist_bert(X_encoded, df['category_level_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist_bert(X_encoded, df['category_level_2'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
